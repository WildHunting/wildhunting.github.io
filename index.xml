<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Wild Hunt</title>
    <link>/</link>
    <description>Recent content on Wild Hunt</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Thu, 01 Jun 2023 16:53:32 +0800</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>cmu 15445 b&#43;树实现</title>
      <link>/posts/cmu15-445/lab2-bplustree/</link>
      <pubDate>Thu, 01 Jun 2023 16:53:32 +0800</pubDate>
      
      <guid>/posts/cmu15-445/lab2-bplustree/</guid>
      <description>1. B+Tree Data Structure 1.1 GetValue() 点查询很简单，略
1.2 Insert() INDEX_TEMPLATE_ARGUMENTS auto BPLUSTREE_TYPE::Insert(const KeyType &amp;amp;key, const ValueType &amp;amp;value, Transaction *transaction) -&amp;gt; bool { if (IsEmpty()) { auto *leaf = reinterpret_cast&amp;lt;LeafPage *&amp;gt;(buffer_pool_manager_-&amp;gt;NewPage(&amp;amp;root_page_id_)-&amp;gt;GetData()); leaf-&amp;gt;Init(root_page_id_, INVALID_PAGE_ID, leaf_max_size_); leaf-&amp;gt;Insert(key, value, comparator_); UpdateRootPageId(root_page_id_); buffer_pool_manager_-&amp;gt;UnpinPage(root_page_id_, true); return true; } auto *l = reinterpret_cast&amp;lt;LeafPage*&amp;gt;(GetLeafPage(key)-&amp;gt;GetData()); if (!l-&amp;gt;Insert(key, value, comparator_)) { buffer_pool_manager_-&amp;gt;UnpinPage(l-&amp;gt;GetPageId(), false); return false; } SolveOverFlow(l); return true; } 如果树为空则从buffer_pool_manager_取出新页，进行init初始化、insert插入、更新root_id、unpin
若不为空，则查找key所在的区间leaf节点，进行插入操作，实际可以使用二分法在节点内搜寻，std::lower_bound有现成的工具函数</description>
    </item>
    
    <item>
      <title>Mvcc</title>
      <link>/posts/etcd/mvcc/</link>
      <pubDate>Tue, 09 May 2023 13:22:15 +0800</pubDate>
      
      <guid>/posts/etcd/mvcc/</guid>
      <description>1.初始化后端存储 1.1 初始化backend // server/etcdserver/server.go line:383 be := openBackend(cfg, beHooks) // server/mvcc/backend/backend.go func newBackend(bcfg BackendConfig) *backend { if bcfg.Logger == nil { bcfg.Logger = zap.NewNop() } bopts := &amp;amp;bolt.Options{} if boltOpenOptions != nil { *bopts = *boltOpenOptions } bopts.InitialMmapSize = bcfg.mmapSize() bopts.FreelistType = bcfg.BackendFreelistType bopts.NoSync = bcfg.UnsafeNoFsync bopts.NoGrowSync = bcfg.UnsafeNoFsync bopts.Mlock = bcfg.Mlock // new db according to bcfg.Path db, err := bolt.Open(bcfg.Path, 0600, bopts) if err != nil { bcfg.Logger.Panic(&amp;quot;failed to open database&amp;quot;, zap.String(&amp;quot;path&amp;quot;, bcfg.</description>
    </item>
    
    <item>
      <title>ETCD RAFT日志复制源码解析</title>
      <link>/posts/etcd/raft-log-replication/</link>
      <pubDate>Fri, 28 Apr 2023 13:54:43 +0800</pubDate>
      
      <guid>/posts/etcd/raft-log-replication/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Update blog</title>
      <link>/posts/update_blog/</link>
      <pubDate>Fri, 14 Apr 2023 21:34:52 +0800</pubDate>
      
      <guid>/posts/update_blog/</guid>
      <description>How to upadte my blog update public first cd blog root directory hugo -D push to github cd public git add -A git commit -m &amp;quot;date&amp;quot; git push -u origin master </description>
    </item>
    
    <item>
      <title>WAL Method</title>
      <link>/posts/etcd/wal-method/</link>
      <pubDate>Fri, 14 Apr 2023 18:51:09 +0800</pubDate>
      
      <guid>/posts/etcd/wal-method/</guid>
      <description>Method&amp;mdash;wal.go Description func Create(lg *zap.Logger, dirpath string, metadata []byte) (*WAL, error) 初次启动raftNode时调用WAL.Create方法。创建WAL对象用于记录追加 ：判断是否存在dirpath路径，如果已存在则不是初次启动raftNode，返回os.ErrExist。创建临时目录和初始上锁的wal文件—walName（seq=0 &amp;amp; index=0），seek到文件末尾（why？），预分配该wal文件大小（SegmentSizeBytes=64MB，优化追加速度），创建WAL对象并设定路径、 metadata（NodeID和ClusterID）、编码器，将上锁的WAL文件追加到锁表内，然后依次写入crc、metadata和空snapshot，重命名临时目录，同步临时目录的父目录（fsync）使得重命名持久化。 func (w *WAL) renameWAL(tmpdirpath string) (*WAL, error) 移除w.dir目录及目录下所有文件和文件夹，调用os.Rename(tmpdirpath, w.dir)将Create方法内创建的临时目录重命名，创建FilePipeline和dirFile *os.File，dirFile is a fd for the wal directory for syncing on Rename func (w *WAL) SaveSnapshot(e walpb.Snapshot) error 检验snapshot是否合法（Since etcd&amp;gt;=3.5.0），pb序列化snapshot得到data字段，加锁，调用w.encoder.encode方法写入record，更新w.enti如果snapshot index &amp;gt; 原w.enti，解锁。 func (w *WAL) saveCrc(prevCrc uint32) error 写入crcType的记录 func (w *WAL) Save(st raftpb.HardState, ents []raftpb.Entry) error 加锁，如果hardstate和entries为空，返回。判断是否需要sync（entries长度不为0||vote改变||term改变），写入entries和hardstate，判断文件当前位置是否小于SegmentSizeBytes（默认64M），如果小于判断是否需要sync数据，如果不小于，返回cut操作结果，解锁。 func (w *WAL) saveEntry(e *raftpb.Entry) error 写入一条entry记录并更新WAL对象的enti值 func (w *WAL) saveState(s *raftpb.</description>
    </item>
    
    <item>
      <title>6.824 lab1 解析</title>
      <link>/posts/6.824/6.824-lab1/</link>
      <pubDate>Fri, 14 Apr 2023 18:43:35 +0800</pubDate>
      
      <guid>/posts/6.824/6.824-lab1/</guid>
      <description>6.824 lab1 笔记 1. 阅读论文 略
2. 官网rules &amp;amp; hints 2.1 rules map阶段每个worker应该把中间文件分成nReduce份，nReduce是reduce任务的数量 worker完成reduce任务后生成文件名mr-out-X mr-out-X文件每行应该是 &amp;quot;%v %v&amp;quot;格式，参考main/mrsequential.go worker处理完map任务，应该把生成的中间文件放到当前目录中，便于worker执行reduce任务时读取中间文件 当所有任务完成时，Done()函数应该返回true，使得coordinator退出 所有任务完成时，worker应该退出，方法是： 当worker调用rpc向coordinator请求任务时，连接不上coordinator，此时可以认为coordinator已经退出因为所有任务已经完成了 当worker调用rpc向coordinator请求任务时，coordinator可以向其回复所有任务已经完成 2.2 hints 刚开始可以修改mr/worker.go&amp;rsquo;s Worker()向coordinator 发送一个RPC请求一个任务。然后修改coordinator回复一个文件名，代表空闲的map任务。然后worker根据文件名读取文件，调用wc.so-Map函数，调用Map函数可参考mrsequential.go
如果修改了 mr/ 目录下任何文件，应该重新build MapReduce plugins，go build -buildmode=plugin ../mrapps/wc.go
worker处理完map任务后产生的中间文件命名格式mr-X-Y，x是map任务的编号，y是reduce任务编号。
// 初始文件，通过命令行传入的，如 // pg-being_ernest.txt pg-dorian_gray.txt pg-frankenstein.txt // len(files) = 3 nReduce = 4 // 中间文件 x：map任务的编号 y：reduce任务编号 // mr-0-0 mr-1-0 mr-2-0 // mr-0-1 mr-1-1 mr-2-1 // mr-0-2 mr-1-2 mr-2-2 // mr-0-3 mr-1-3 mr-2-3 map任务存储数据到文件可以使用json格式，便于reduce任务读取
// map enc := json.NewEncoder(file) for _, kv := .</description>
    </item>
    
    <item>
      <title>ETCD RAFT选举源码解析</title>
      <link>/posts/etcd/raft-leader-election/</link>
      <pubDate>Wed, 12 Apr 2023 21:45:37 +0800</pubDate>
      
      <guid>/posts/etcd/raft-leader-election/</guid>
      <description>ETCD-raft笔记 0. 引言 该篇博客基于etcd v3.5.7版本，首先会简单介绍etcd/raft对Raft选举部分的算法优化，然后通过源码分析etcd/raft的选举实现。
1. etcd对于raft选举算法优化措施 该优化措施均在raft博士论文中有讲解
etcd/raft实现的与选举有关的优化有Pre-Vote、Check Quorum、和Leader Lease。在这三种优化中，只有Pre-Vote和Leader Lease最初是对选举过程的优化，Check Quorum是为了更高效地实现线性一致性读（Linearizable Read）而做出的优化，但是由于Leader Lease需要依赖Check Quorum，因此也放在这讲。
1.1 Pre-Vote 如下图所示，当Raft集群的网络发生分区时，会出现节点数达不到quorum（达成共识至少需要的节点数）的分区，如图中的Partition 1。
在节点数能够达到quorum的分区中，选举流程会正常进行，该分区中的所有节点的term最终会稳定为新选举出的leader节点的term。不幸的是，在节点数无法达到quorum的分区中，如果该分区中没有leader节点，因为节点总是无法收到数量达到quorum的投票而不会选举出新的leader，所以该分区中的节点在election timeout超时后，会增大term并发起下一轮选举，这导致该分区中的节点的term会不断增大。
如果网络一直没有恢复，这是没有问题的。但是，如果网络分区恢复，此时，达不到quorum的分区中的节点的term值会远大于能够达到quorum的分区中的节点的term，这会导致能够达到quorum的分区的leader退位（step down）并增大自己的term到更大的term，使集群产生一轮不必要的选举。
Pre-Vote机制就是为了解决这一问题而设计的，其解决的思路在于不允许达不到quorum的分区正常进入投票流程，也就避免了其term号的增大。为此，Pre-Vote引入了“预投票”，也就是说，当节点election timeout超时时，它们不会立即增大自身的term并请求投票，而是先发起一轮预投票。收到预投票请求的节点不会退位。只有当节点收到了达到quorum的预投票响应时，节点才能增大自身term号并发起投票请求。这样，达不到quorum的分区中的节点永远无法增大term，也就不会在分区恢复后引起不必要的一轮投票。
1.2 Check Quorum 在Raft算法中，保证线性一致性读取的最简单的方式，就是讲读请求同样当做一条Raft提议，通过与其它日志相同的方式执行，因此这种方式也叫作Log Read。显然，Log Read的性能很差。而在很多系统中，读多写少的负载是很常见的场景。因此，为了提高读取的性能，就要试图绕过日志机制。
但是，直接绕过日志机制从leader读取，可能会读到陈旧的数据，也就是说存在stale read的问题。在下图的场景中，假设网络分区前，Node 5是整个集群的leader。在网络发生分区后，Partition 0分区中选举出了新leader，也就是图中的Node 1。
但是，由于网络分区，Node 5无法收到Partition 0中节点的消息，Node 5不会意识到集群中出现了新的leader。此时，虽然它不能成功地完成日志提交，但是如果读取时绕过了日志，它还是能够提供读取服务的。这会导致连接到Node 5的client读取到陈旧的数据。
Check Quorum可以减轻这一问题带来的影响，其机制也非常简单：让leader每隔一段时间主动地检查follower是否活跃。如果活跃的follower数量达不到quorum，那么说明该leader可能是分区前的旧leader，所以此时该leader会主动退位转为follower。
需要注意的是，Check Quorum并不能完全避免stale read的发生，只能减小其发生时间，降低影响。如果需要严格的线性一致性，需要通过其它机制实现。
1.3 Leader Lease 分布式系统中的网络环境十分复杂，有时可能出现网络不完全分区的情况，即整个整个网络拓补图是一个连通图，但是可能并非任意的两个节点都能互相访问。
这种现象不止会出现在网络故障中，还会出现在成员变更中。在通过ConfChange移除节点时，不同节点应用该ConfChange的时间可能不同，这也可能导致这一现象发生——TODO （举个例子）。
在上图的场景下，Node 1与Node 2之间无法通信。如果它们之间的通信中断前，Node 1是集群的leader，在通信中断后，Node 2无法再收到来自Node 1的心跳。因此，Node 2会开始选举。如果在Node 2发起选举前，Node 1和Node 3中都没有新的日志，那么Node 2仍可以收到能达到quorum的投票（来自Node 2本身的投票和来自Node 3的投票），并成为leader。
Leader Lease机制对投票引入了一条新的约束以解决这一问题：当节点在election timeout超时前，如果收到了leader的消息，那么它不会为其它发起投票或预投票请求的节点投票。也就是说，Leader Lease机制会阻止了正常工作的集群中的节点给其它节点投票。
Leader Lease需要依赖Check Quorum机制才能正常工作。接下来笔者通过一个例子说明其原因。
假如在一个5个节点组成的Raft集群中，出现了下图中的分区情况：Node 1与Node 2互通，Node 3、Node 4、Node 5之间两两互通、Node 5与任一节点不通。在网络分区前，Node 1是集群的leader。</description>
    </item>
    
    <item>
      <title>Chapter 1</title>
      <link>/docs/example-doc/chapter-1/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/docs/example-doc/chapter-1/</guid>
      <description>&lt;p&gt;This is chapter 1 of example doc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 1</title>
      <link>/docs/example-doc/nested-chapter/chapter-1/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/docs/example-doc/nested-chapter/chapter-1/</guid>
      <description>&lt;p&gt;This is chapter 1 of nested chapter.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2</title>
      <link>/docs/example-doc/chapter-2/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/docs/example-doc/chapter-2/</guid>
      <description>&lt;p&gt;This is chapter 2 of example doc.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Chapter 2</title>
      <link>/docs/example-doc/nested-chapter/chapter-2/</link>
      <pubDate>Fri, 16 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/docs/example-doc/nested-chapter/chapter-2/</guid>
      <description>&lt;p&gt;This is chapter 2 of nested chapter.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>
